#Please note, code use to import data is for Kaggle notebook. For other notebooks dataset importing and unzipping code will be slightly different.

!mkdir train

import bz2

# Specify the path to the .bz2 file
file_path = '/kaggle/input/amazonreviews/test.ft.txt.bz2'

# Specify the output file name and path
output_file = '/kaggle/working/train/test.text'

# Open the .bz2 file for reading
with bz2.open(file_path, 'rb') as source_file:
    # Open the output file for writing
    with open(output_file, 'wb') as output:
        # Read and write the contents of the .bz2 file
        output.write(source_file.read())

print("Unzipping complete.")

# Specify the path to the .bz2 file
file_path = '/kaggle/input/amazonreviews/train.ft.txt.bz2'

# Specify the output file name and path
output_file = '/kaggle/working/train/train.text'

# Open the .bz2 file for reading
with bz2.open(file_path, 'rb') as source_file:
    # Open the output file for writing
    with open(output_file, 'wb') as output:
        # Read and write the contents of the .bz2 file
        output.write(source_file.read())

print("Unzipping complete.")
import pandas as pd
import numpy as np
import pandas as pd

# Specify the path to the dataset file
file_path1 = '/kaggle/working/train/train.text'

# Read the dataset file with explicit delimiter and number of columns
df = pd.read_csv(file_path, delimiter='\t', header=None, names=['Sentiment'], quoting=3, error_bad_lines=False)

# Split the 'Sentiment' column into 'Label' and 'Review' columns
df[['Label', 'Review']] = df['Sentiment'].str.split(' ', 1, expand=True)

# Remove the original 'Sentiment' column
df.drop('Sentiment', axis=1, inplace=True)

# select 10000 rows
df = df.sample(n=10000, random_state=50)

# Display the first few rows of the DataFrame
print(df.head())
print(df.shape)

import pandas as pd

# Specify the path to the dataset file
file_path2 = '/kaggle/working/train/test.text'

# Read the dataset file with explicit delimiter and number of columns
df2 = pd.read_csv(file_path2, delimiter='\t', header=None, names=['Sentiment'], quoting=3, error_bad_lines=False)

# Split the 'Sentiment' column into 'Label' and 'Review' columns
df2[['Label', 'Review']] = df2['Sentiment'].str.split(' ', 1, expand=True)

# Remove the original 'Sentiment' column
df2.drop('Sentiment', axis=1, inplace=True)

# select 10000 rows
df2 = df.sample(n=10000, random_state=50)

# Display the first few rows of the DataFrame
print(df2.head())
print(df2.shape)

df['Label'] = df['Label'].replace('__label__2', '2')
df['Label'] = df['Label'].replace('__label__1', '1')
df2['Label'] = df2['Label'].replace('__label__2', '2')
df2['Label'] = df2['Label'].replace('__label__1', '1')

print(df.head())
print(df2.head())
import torch
from tqdm.notebook import tqdm
# to check the counts of the labels
df.Label.value_counts()
# to check the counts of the labels
df2.Label.value_counts()
set(df.Label)
possible_labels=df.Label.unique()
possible_labels
label_dict={}
for index,possible_label in enumerate(possible_labels):
    label_dict[possible_label]=index

label_dict
df.Label=df['Label'].map(label_dict)
df.Label.info()
df2.Label=df2['Label'].map(label_dict)
df2.Label.info()
X_train=df['Review']
y_train=df['Label']
X_train.shape
y_train.shape
X_test=df2['Review']
y_test=df2['Label']
X_test.shape
y_test.shape
!pip install transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset
tokenizer=BertTokenizer.from_pretrained('bert-large-uncased',
                                        do_lower_case=True)
df['data_type'] = ['train']*df.shape[0]
df2['data_type'] = ['val']*df2.shape[0]
df2.head()
# Encoding train data
encoded_data_train = tokenizer.batch_encode_plus(
    df[df['data_type'] == 'train'].Review.values,
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=256,
    truncation=True,
    return_tensors='pt'
)

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df[df['data_type'] == 'train'].Label.values.astype(int))

# Encoding validation data
encoded_data_val = tokenizer.batch_encode_plus(
    df2[df2['data_type'] == 'val'].Review.values,
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=256,
    truncation=True,
    return_tensors='pt'
)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(df2[df2['data_type'] == 'val'].Label.values.astype(int))

input_ids_val
#TensorDataset create a single variable which stores info of inputids , attentionmsk, labels
dataset_train = TensorDataset(input_ids_train, 
                              attention_masks_train,
                              labels_train)

dataset_val = TensorDataset(input_ids_val, 
                            attention_masks_val,
                           labels_val)
len(dataset_train)
dataset_train.tensors
**Setting up BERT Pretrained model**
from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained(
                                      'bert-large-uncased', 
                                      num_labels = len(label_dict),
                                      output_attentions = False,
                                      output_hidden_states = False
                                     )
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
batch_size = 4

dataloader_train = DataLoader(
    dataset_train,
    sampler=RandomSampler(dataset_train),
    batch_size=batch_size
)

dataloader_val = DataLoader(
    dataset_val,
    sampler=RandomSampler(dataset_val),
    batch_size=32
)
from transformers import AdamW, get_linear_schedule_with_warmup
optimizer = AdamW(
    model.parameters(),
    lr = 1e-5,
    eps = 1e-8
)
epochs = 1

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps = len(dataloader_train)*epochs
)
**Defining our Performance Metrics**
import numpy as np
from sklearn.metrics import f1_score
def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average = 'weighted')
def accuracy_per_class(preds, labels):
    label_dict_inverse = {v: k for k, v in label_dict.items()}
    
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    
    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\n')
**Creating our Training Loop**
import random

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)
def evaluate(dataloader_val):

    model.eval()
    
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in tqdm(dataloader_val):
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals
for epoch in tqdm(range(1, epochs+1)):
    model.train() #forward propagation
    loss_train_total = 0
    
    progress_bar = tqdm(dataloader_train, 
                        desc='Epoch {:1d}'.format(epoch), 
                        leave=False, 
                        disable=False)
    
    for batch in progress_bar:
        model.zero_grad()
        batch = tuple(b.to(device) for b in batch)
        inputs = {
            'input_ids': batch[0],
            'attention_mask': batch[1],
            'labels': batch[2]
        }
        
        outputs = model(**inputs)
        loss = outputs[0]
        loss_train_total +=loss.item()
        loss.backward() #backwardprop
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     
    
    torch.save(model, f'BERT_ft_Epoch{epoch}.model')
    
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_val)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (weighted): {val_f1}')

torch.save(model.state_dict(), 'Sentiment_Analysis_Amazon_Review.pt')
test_sentence = "The jacket fit nicely on me and the colour was also as expected. But there was a yellow stain near the front pocket of the jacket which if inspected closely looked like a food stain. This made me seriously doubt if they have given me a previously used and returned product. The stitches of some places are undone even before I tried wearing it. So I had to redo it. But I didn't return the product because its price is not so high and it is sufficient for its price."

predict_input = tokenizer.batch_encode_plus(
    [test_sentence],
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=256,
    truncation=True,
    return_tensors='pt'
)

predict_input
input_ids = predict_input['input_ids'].to(device)
attention_mask = predict_input['attention_mask'].to(device)

with torch.no_grad():
    outputs = model(input_ids, attention_mask)

logits = outputs.logits
predictions = torch.argmax(logits, dim=1)

predicted_label = predictions.item()

Senti_label_dict = {'0': 'Good Review', '1': 'Bad Review'}
predicted_sentiment = Senti_label_dict.get(str(predicted_label))
print(predicted_sentiment)



# #If want to load the model (Some times worked) 
import torch
from transformers import BertForSequenceClassification, BertConfig

config = BertConfig.from_pretrained('bert-large-uncased', num_labels=len(label_dict))
model = BertForSequenceClassification.from_pretrained('bert-large-uncased', config=config)
